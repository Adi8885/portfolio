{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report , accuracy_score , confusion_matrix , precision_score , f1_score\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv #GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import to_networkx , from_networkx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classification_report(y_actual , y_pred):\n",
    "    \"\"\"\n",
    "    Description : \n",
    "    - This function takes precdicted and actual values and gives key classification metrics\n",
    "    \n",
    "    Inputs:\n",
    "    1. y_actual : Array of actual labels\n",
    "    2. y_pred :  Array of predicted labels\n",
    "    \n",
    "    Outputs:\n",
    "    1. Prescision , recall, F1-Score and Accuracy metrics\n",
    "    2. Plot of confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    print(classification_report(y_actual, y_pred))\n",
    "    acc = accuracy_score(y_actual , y_pred)\n",
    "    print('accuracy : {}'.format(acc))\n",
    "   \n",
    "    conf_mat=confusion_matrix(y_actual, y_pred)\n",
    "    print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "==================================================\n",
      "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Get some basic info about the dataset\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(50*'=')\n",
    "\n",
    "# There is only one graph in the dataset, use it as new data object\n",
    "data = dataset[0]  \n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(data)\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (out): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Initialize the layers\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.out = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First Message Passing Layer (Transformation)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second Message Passing Layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Output layer \n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = GCN(hidden_channels=64)\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "# Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "# probability distributions)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad() \n",
    "    # Use all data as input, because all nodes have node features\n",
    "    out = model(data.x, data.edge_index)  \n",
    "    # Only use nodes with labels available for loss calculation --> mask\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # Use the class with highest probability.\n",
    "    pred = out.argmax(dim=1)  \n",
    "    # Check against ground-truth labels.\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  \n",
    "    # Derive ratio of correct predictions.\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "    return pred , test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.945961594581604 , Time : 0.13 secs\n",
      "Epoch: 200, Loss: 1.2611536979675293 , Time : 4.81 secs\n",
      "Epoch: 400, Loss: 1.2241952419281006 , Time : 4.3 secs\n",
      "Epoch: 600, Loss: 1.2124096155166626 , Time : 4.36 secs\n",
      "Epoch: 800, Loss: 1.2027983665466309 , Time : 4.35 secs\n",
      "Epoch: 1000, Loss: 1.2085927724838257 , Time : 4.42 secs\n",
      "Epoch: 1200, Loss: 1.2158174514770508 , Time : 4.38 secs\n",
      "Epoch: 1400, Loss: 1.1976265907287598 , Time : 4.33 secs\n",
      "Epoch: 1600, Loss: 1.1997929811477661 , Time : 4.41 secs\n",
      "Epoch: 1800, Loss: 1.2116413116455078 , Time : 4.35 secs\n",
      "Epoch: 2000, Loss: 1.2076278924942017 , Time : 4.39 secs\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "start_time = time.time()\n",
    "for epoch in range(0, 2001):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 200 == 0:\n",
    "        print('Epoch: {}, Loss: {} , Time : {} secs'.format(epoch , loss , round(time.time() - start_time , 2)))\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.67      0.69       130\n",
      "           1       0.70      0.86      0.77        91\n",
      "           2       0.89      0.86      0.87       144\n",
      "           3       0.88      0.79      0.83       319\n",
      "           4       0.70      0.80      0.74       149\n",
      "           5       0.80      0.78      0.79       103\n",
      "           6       0.73      0.81      0.77        64\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.77      0.79      0.78      1000\n",
      "weighted avg       0.80      0.79      0.79      1000\n",
      "\n",
      "accuracy : 0.791\n",
      "[[ 87   8   2  10  12   3   8]\n",
      " [  3  78   5   3   0   1   1]\n",
      " [  2   9 124   6   3   0   0]\n",
      " [  7  10   5 251  37   8   1]\n",
      " [ 11   5   0   9 119   3   2]\n",
      " [  6   1   4   5   0  80   7]\n",
      " [  5   0   0   2   0   5  52]]\n"
     ]
    }
   ],
   "source": [
    "my_classification_report(y_actual = data.y[data.test_mask].detach().cpu().numpy()\n",
    "                         , y_pred = pred[data.test_mask].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0500, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0500, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[data.train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(data.x[data.train_mask].detach().cpu().numpy() \n",
    "          , data.y[data.train_mask].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data.x[data.test_mask].detach().cpu().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.42      0.47       130\n",
      "           1       0.55      0.75      0.64        91\n",
      "           2       0.49      0.81      0.61       144\n",
      "           3       0.81      0.51      0.63       319\n",
      "           4       0.48      0.58      0.53       149\n",
      "           5       0.60      0.49      0.53       103\n",
      "           6       0.49      0.56      0.52        64\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.56      0.59      0.56      1000\n",
      "weighted avg       0.61      0.57      0.57      1000\n",
      "\n",
      "accuracy : 0.573\n",
      "[[ 54  14  23   5  18   5  11]\n",
      " [  5  68   4   8   2   2   2]\n",
      " [  4   6 116   8   4   1   5]\n",
      " [ 17   9  48 163  57  16   9]\n",
      " [  8   7  25  14  86   7   2]\n",
      " [  6  12  16   4   6  50   9]\n",
      " [  6   7   7   0   5   3  36]]\n"
     ]
    }
   ],
   "source": [
    "my_classification_report(y_actual = data.y[data.test_mask].detach().cpu().numpy()\n",
    "                         , y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with traditional simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=1433, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size , hidden_size , output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.fc1 = nn.Linear(in_features = input_size , out_features = hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features = hidden_size , out_features = hidden_size)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(in_features = hidden_size , out_features = output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #Pass output of BERT through Fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        #Output layer\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x , dim = 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = SimpleNN(input_size = dataset.num_features , hidden_size = 64, output_size = dataset.num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "# probability distributions)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad() \n",
    "    # Use all data as input, because all nodes have node features\n",
    "    out = model(data.x.unsqueeze(0))  \n",
    "    # Only use nodes with labels available for loss calculation --> mask\n",
    "    loss = criterion(out[: ,data.train_mask].squeeze(0) , data.y[data.train_mask])  \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x.unsqueeze(0)) \n",
    "    # Use the class with highest probability.\n",
    "    pred = out.argmax(dim=2)  \n",
    "    # Check against ground-truth labels.\n",
    "    test_correct = pred[: , data.test_mask] == data.y[data.test_mask]  \n",
    "    # Derive ratio of correct predictions.\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "    return pred , test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.9459106922149658 , Time : 0.02 secs\n",
      "Epoch: 100, Loss: 1.9038491249084473 , Time : 1.12 secs\n",
      "Epoch: 200, Loss: 1.9037597179412842 , Time : 1.13 secs\n",
      "Epoch: 300, Loss: 1.9036071300506592 , Time : 1.13 secs\n",
      "Epoch: 400, Loss: 1.903564691543579 , Time : 1.15 secs\n",
      "Epoch: 500, Loss: 1.9035533666610718 , Time : 1.13 secs\n",
      "Epoch: 600, Loss: 1.9035528898239136 , Time : 1.1 secs\n",
      "Epoch: 700, Loss: 1.9035208225250244 , Time : 1.15 secs\n",
      "Epoch: 800, Loss: 1.9035139083862305 , Time : 1.13 secs\n",
      "Epoch: 900, Loss: 1.9035062789916992 , Time : 1.13 secs\n",
      "Epoch: 1000, Loss: 1.9034757614135742 , Time : 1.14 secs\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "start_time = time.time()\n",
    "for epoch in range(0, 1001):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: {}, Loss: {} , Time : {} secs'.format(epoch , loss , round(time.time() - start_time , 2)))\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred , test_acc = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.10      0.16       130\n",
      "           1       0.24      0.55      0.33        91\n",
      "           2       0.18      0.58      0.27       144\n",
      "           3       0.47      0.12      0.19       319\n",
      "           4       0.43      0.06      0.11       149\n",
      "           5       0.17      0.18      0.18       103\n",
      "           6       0.21      0.28      0.24        64\n",
      "\n",
      "    accuracy                           0.23      1000\n",
      "   macro avg       0.31      0.27      0.21      1000\n",
      "weighted avg       0.35      0.23      0.20      1000\n",
      "\n",
      "accuracy : 0.23\n",
      "[[ 13  31  59   8   3   7   9]\n",
      " [  3  50  22   6   0   9   1]\n",
      " [  3  18  84   6   4  12  17]\n",
      " [  5  59 144  37   3  46  25]\n",
      " [  3  23  79  13   9  10  12]\n",
      " [  2  16  55   8   1  19   2]\n",
      " [  0  14  25   0   1   6  18]]\n"
     ]
    }
   ],
   "source": [
    "my_classification_report(y_actual = data.y[data.test_mask].detach().cpu().numpy()\n",
    "                         , y_pred = pred[:,data.test_mask].squeeze(0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
